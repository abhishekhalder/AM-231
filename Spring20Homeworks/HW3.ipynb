{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [70 points] Rate-of-convergence of algorithmic flows in machine learning\n",
    "\n",
    "The condition $\\dot{V}\\leq 0$ appearing in Lyapunov-style theorems simply means that $V(t)$ decays monotonically along each trajectory of the nonlinear dynamical system. This idea itself is powerful even outside the context of Lyapunov function. For example, if one wants to derive the rate-of-convergence of popular machine learning algorithms then one can construct a function $E(t)$ that decreases **along the flow of the algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [15 + 20 = 35 points] Rate of convergence of gradient flow\n",
    "\n",
    "Consider the continuous-time version of the popular gradient descent algorithm called \"gradient flow\", given by\n",
    "\n",
    "$$\\dot{x} = -\\nabla f, \\quad x\\in\\mathbb{R}^{n},\\quad f\\;\\text{is convex and}\\;C^{1}.$$\n",
    "\n",
    "To say $f$ is [convex](https://en.wikipedia.org/wiki/Convex_function) and $C^{1}$ function, is same as saying\n",
    "\n",
    "$$f(y) \\geq f(x) + \\langle \\nabla f(x), y-x\\rangle \\quad \\text{for all}\\; x,y\\;\\text{in the domain of}\\; f.$$\n",
    "\n",
    "Recognizing that the RHS of the above inequality is the first order Taylor series approximation of $f$ about the point $x$, we get a precise geometric interpretation: in its entire domain, the function $f$ lies above its linear approximation (tangent hyperplane).\n",
    "\n",
    "(i) Let $x^{\\rm{opt}}$ be a minimizer of the function $f$, and use the notation $f^{\\rm{opt}} := f(x^{\\rm{opt}})$. Consider the function $E(t) = t(f(x) - f^{\\rm{opt}}) + \\frac{1}{2}\\|x - x^{\\rm{opt}}\\|_{2}^{2}$ for $t\\geq 0$. **Prove that** $\\dot{E} \\leq 0$ for the gradient flow.\n",
    "\n",
    "(ii) Use your answer in part (i) to **show that** the gradient flow converges at rate $O(1/t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [20 + 15 = 35 points] Rate of convergence of Nesterov's accelerated gradient flow\n",
    "\n",
    "Consider the continuous-time version of the popular Nesterov's accelearted gradient (NAG) descent algorithm called \"NAG flow\", given by\n",
    "\n",
    "$$\\ddot{x} + \\frac{r+1}{t}\\dot{x} + \\nabla f(x) = 0, \\quad x\\in\\mathbb{R}^{n},\\quad f\\;\\text{is convex and}\\;C^{1},\\quad r\\;\\text{is a constant parameter.}$$\n",
    "\n",
    "(i) Consider the function $E(t) = \\frac{t^2}{r}(f(x) - f^{\\rm{opt}}) + \\frac{r}{2}\\|x + \\frac{t}{r}\\dot{x} - x^{\\rm{opt}}\\|_{2}^{2}$ for $t\\geq 0$. **Prove that** $\\dot{E} \\leq 0$ for the NAG flow provided $r\\geq 2$. (Hint: $f \\geq f^{\\rm{opt}}$ for convex $f$)\n",
    "\n",
    "(ii) Use your answer in part (i) to **show that** the NAG flow with $r\\geq 2$ converges at rate $O(1/t^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 2. [10 x 3 = 30 points] Input-to-state stability\n",
    "\n",
    "Which of the following **scalar** nonlinear systems are input-to-state stable (ISS) and which are not? Give reasons to support your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) $\\dot{x} = -(1+u)x^3$\n",
    "\n",
    "(ii) $\\dot{x} = -(1+u)x^3 - x^5$\n",
    "\n",
    "(iii) $\\dot{x} = - x + x^2 u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
