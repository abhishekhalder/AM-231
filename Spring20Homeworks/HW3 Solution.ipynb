{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [70 points] Rate-of-convergence of algorithmic flows in machine learning\n",
    "\n",
    "The condition $\\dot{V}\\leq 0$ appearing in Lyapunov-style theorems simply means that $V(t)$ decays monotonically along each trajectory of the nonlinear dynamical system. This idea itself is powerful even outside the context of Lyapunov function. For example, if one wants to derive the rate-of-convergence of popular machine learning algorithms then one can construct a function $E(t)$ that decreases **along the flow of the algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [15 + 20 = 35 points] Rate of convergence of gradient flow\n",
    "\n",
    "Consider the continuous-time version of the popular gradient descent algorithm called \"gradient flow\", given by\n",
    "\n",
    "$$\\dot{x} = -\\nabla f, \\quad x\\in\\mathbb{R}^{n},\\quad f\\;\\text{is convex and}\\;C^{1}.$$\n",
    "\n",
    "To say $f$ is [convex](https://en.wikipedia.org/wiki/Convex_function) and $C^{1}$ function, is same as saying\n",
    "\n",
    "$$f(y) \\geq f(x) + \\langle \\nabla f(x), y-x\\rangle \\quad \\text{for all}\\; x,y\\;\\text{in the domain of}\\; f.$$\n",
    "\n",
    "Recognizing that the RHS of the above inequality is the first order Taylor series approximation of $f$ about the point $x$, we get a precise geometric interpretation: in its entire domain, the function $f$ lies above its linear approximation (tangent hyperplane).\n",
    "\n",
    "(i) Let $x^{\\rm{opt}}$ be a minimizer of the function $f$, and use the notation $f^{\\rm{opt}} := f(x^{\\rm{opt}})$. Consider the function $E(t) = t(f(x) - f^{\\rm{opt}}) + \\frac{1}{2}\\|x - x^{\\rm{opt}}\\|_{2}^{2}$ for $t\\geq 0$. **Prove that** $\\dot{E} \\leq 0$ for the gradient flow.\n",
    "\n",
    "(ii) Use your answer in part (i) to **show that** the gradient flow converges at rate $O(1/t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for 1(a):\n",
    "\n",
    "(i) We compute\n",
    "$$\\begin{align*}\n",
    "\\dot{E} &= \\frac{\\partial E}{\\partial t} + \\langle\\nabla_{x} E, f\\rangle\\\\\n",
    "&= f - f^{\\text{opt}} - t\\|\\nabla_{x} f\\|_{2}^{2} + \\langle x^{\\text{opt}} - x, \\nabla_{x}f\\rangle\\\\\n",
    "&= \\underbrace{\\{f - f^{\\text{opt}} + \\langle x^{\\text{opt}} - x, \\nabla_{x}f\\rangle\\}}_{\\leq 0} - \\underbrace{t\\|\\nabla_{x} f\\|_{2}^{2}}_{\\geq 0}\\\\\n",
    "&\\leq 0,\n",
    "\\end{align*}$$\n",
    "\n",
    "where the inequality in the last line follows from the convexity of $f$ (substitute $y\\equiv x^{\\text{opt}}$ in the inequality given in the problem statement). This completes the proof.\n",
    "\n",
    "(ii) From part (i), we know that starting from any initial condition $x_{0} := x(0)$, the function $E(t)$ is decreasing along the flow $x(t)$ for all $t>0$. Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "E(t) &\\leq E(0), \\\\\n",
    "\\Rightarrow t\\left(f(x) - f^{\\text{opt}}\\right) &\\leq \\frac{1}{2}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2} - \\frac{1}{2}\\|x - x^{\\text{opt}}\\|_{2}^{2} \\leq \\frac{1}{2}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2},\\\\\n",
    "\\Rightarrow f(x) - f^{\\text{opt}} &\\leq \\frac{1}{2t}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2},\n",
    "\\end{align*}$$\n",
    "\n",
    "and therefore, $f(x) - f^{\\text{opt}} = O(1/t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [20 + 15 = 35 points] Rate of convergence of Nesterov's accelerated gradient flow\n",
    "\n",
    "Consider the continuous-time version of the popular Nesterov's accelearted gradient (NAG) descent algorithm called \"NAG flow\", given by\n",
    "\n",
    "$$\\ddot{x} + \\frac{r+1}{t}\\dot{x} + \\nabla f(x) = 0, \\quad x\\in\\mathbb{R}^{n},\\quad f\\;\\text{is convex and}\\;C^{1},\\quad r\\;\\text{is a constant parameter.}$$\n",
    "\n",
    "(i) Consider the function $E(t) = \\frac{t^2}{r}(f(x) - f^{\\rm{opt}}) + \\frac{r}{2}\\|x + \\frac{t}{r}\\dot{x} - x^{\\rm{opt}}\\|_{2}^{2}$ for $t\\geq 0$. **Prove that** $\\dot{E} \\leq 0$ for the NAG flow provided $r\\geq 2$. (Hint: $f \\geq f^{\\rm{opt}}$ for convex $f$)\n",
    "\n",
    "(ii) Use your answer in part (i) to **show that** the NAG flow with $r\\geq 2$ converges at rate $O(1/t^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for 1(b):\n",
    "\n",
    "(i) Rewriting the dynamics in the state space form in $\\mathbb{R}^{2n}$, we get\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "\\dot{x}_{1}\\\\\n",
    "\\dot{x}_{2}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "x_{2}\\\\\n",
    "-\\nabla_{x_{1}}f(x_{1}) - \\frac{r+1}{t}x_{2}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "wherein $x_{1}:=x$, and $x_{2}:=\\dot{x}$. We compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial t} &= \\frac{2t}{r}\\left(f - f^{\\text{opt}}\\right) + \\bigg\\langle x_{1} + \\frac{t}{r}x_{2} - x_{1}^{\\text{opt}}, x_{2}\\bigg\\rangle,\\\\\n",
    "\\nabla_{x_{1}}E &= \\frac{t^{2}}{r}\\nabla_{x_{1}}f + r\\left(x_{1} + \\frac{t}{r}x_{2} - x_{1}^{\\text{opt}}\\right),\\\\\n",
    "\\nabla_{x_{2}}E &= t\\left(x_{1} + \\frac{t}{r}x_{2} - x_{1}^{\\text{opt}}\\right),\n",
    "\\end{align*}$$\n",
    "\n",
    "and therefore,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\dot{E} &= \\frac{\\partial E}{\\partial t} + \\langle\\nabla_{x_{1}} E, x_{2}\\rangle + \\bigg\\langle\\nabla_{x_{2}} E, -\\nabla_{x_{1}}f - \\frac{r+1}{t}x_{2}\\bigg\\rangle\\\\\n",
    "&= \\frac{2t}{r}\\left(f - f^{\\text{opt}}\\right) + t\\langle x_{1}^{\\text{opt}} - x_{1}, \\nabla_{x_{1}}f\\rangle\\\\\n",
    "&= \\underbrace{t\\left(f - f^{\\text{opt}} + \\langle x^{\\text{opt}} - x, \\nabla_{x}f\\rangle\\right)}_{\\leq 0} + \\underbrace{t\\left(\\frac{2-r}{r}\\right)\\left(f-f^{\\text{opt}}\\right)}_{\\leq 0}\\\\\n",
    "&\\leq 0,\n",
    "\\end{align*}$$\n",
    "\n",
    "where the inequalities in the last but one line follows from the convexity of $f$ (substitute $y\\equiv x^{\\text{opt}}$ in the inequality given in the problem statement), and from the facts that $r \\geq 2$, $f\\geq f^{\\text{opt}}$. This completes the proof.\n",
    "\n",
    "(ii) From part (i), we know that starting from any initial condition $x_{0} := x(0)$, the function $E(t)$ is decreasing along the flow $x(t)$ for all $t>0$. Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "E(t) &\\leq E(0), \\\\\n",
    "\\Rightarrow \\frac{t^2}{r}\\left(f(x) - f^{\\text{opt}}\\right) &\\leq \\frac{r}{2}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2} - \\frac{r}{2}\\|x - x^{\\text{opt}}\\|_{2}^{2} \\leq \\frac{r}{2}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2},\\\\\n",
    "\\Rightarrow f(x) - f^{\\text{opt}} &\\leq \\frac{r^2}{2t^2}\\|x_{0} - x^{\\text{opt}}\\|_{2}^{2},\n",
    "\\end{align*}$$\n",
    "\n",
    "and therefore, $f(x) - f^{\\text{opt}} = O(1/t^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 2. [10 x 3 = 30 points] Input-to-state stability\n",
    "\n",
    "Which of the following **scalar** nonlinear systems are input-to-state stable (ISS) and which are not? Give reasons to support your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) $\\dot{x} = -(1+u)x^3$\n",
    "\n",
    "(ii) $\\dot{x} = -(1+u)x^3 - x^5$\n",
    "\n",
    "(iii) $\\dot{x} = - x + x^2 u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for 2:\n",
    "\n",
    "(i) Not ISS since $u(t)\\equiv \\text{constant} > 1$ with $x_{0} > 0$ leads to $\\displaystyle\\lim_{t\\rightarrow\\infty}x(t) = \\infty$.\n",
    "\n",
    "(ii) Let $V(x)=\\frac{1}{2}x^{2}$. Then\n",
    "\n",
    "$$\\dot{V}=-x^{4} + ux^{4} - x^{6} \\leq -x^{4}, \\quad\\text{for all}\\quad |x|>\\sqrt{u}.$$\n",
    "\n",
    "Invoking the ISS Lyapunov theorem (Lec. 8, p. 14-15) with $\\alpha_{1}\\left(|x|\\right) = \\alpha_{2}\\left(|x|\\right)=V(x)$, $W_{3}(|x|) = x^4$, and $\\rho(|u|)=\\sqrt{u}$, we conclude that the system is ISS with $\\gamma(r)=\\rho(r)=\\sqrt{r}$.\n",
    "\n",
    "(iii) Not ISS since $u(t)\\equiv 1$ with $x_{0}>0$ leads to $\\displaystyle\\lim_{t\\rightarrow\\infty}x(t) = \\infty$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
